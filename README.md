# OpenAI Streaming Chat Comparison

This is a Next.js project that allows you to compare responses from two different models served via the Requesty.ai router. It streams responses in real-time and displays stats such as TTFB, total time, and token counts.

## Features

- Compare two models side-by-side.
- Real-time streaming responses.
- Display token usage and timing stats.

## Requirements

- Node.js (version 16 or higher recommended)
- A Requesty API key (for `INSIGHT_API_KEY`).

## Getting Started

1. **Clone the repo:**
   ```bash
   git clone https://github.com/yourusername/my-openai-streaming-chat.git
   cd my-openai-streaming-chat
# LLM-Comparison
